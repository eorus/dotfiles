#!/usr/bin/env bash
# neomutt-extract-urls
# Extract URLs from an email on stdin and append unique ones to ~/Documents/neomutt-urls.txt
set -euo pipefail

OUTFILE="${HOME}/Documents/neomutt-urls.txt"
TMPDIR="$(mktemp -d "${TMPDIR:-/tmp}/neomutt-urls-XXXXXX")"
cleanup() { rm -rf "$TMPDIR"; }
trap cleanup EXIT

stdin_file="$TMPDIR/message.eml"
cat - > "$stdin_file"

# crude URL regex (http/https/ftp/mailto)
if command -v rg >/dev/null 2>&1; then
  rg -o --no-line-number 'https?://[^\s<>()]+' "$stdin_file" > "$TMPDIR/urls.txt" || true
else
  # fallback to grep+sed
  grep -oE 'https?://[^[:space:]<>()]+' "$stdin_file" > "$TMPDIR/urls.txt" || true
fi

if [ ! -s "$TMPDIR/urls.txt" ]; then
  echo "No URLs found." >&2
  exit 0
fi

mkdir -p "$(dirname "$OUTFILE")"
# Append unique new URLs
if [ -f "$OUTFILE" ]; then
  grep -Fvx -f "$OUTFILE" "$TMPDIR/urls.txt" >> "$OUTFILE" || true
else
  cat "$TMPDIR/urls.txt" >> "$OUTFILE"
fi

# Deduplicate file in-place (portable)
awk '!seen[$0]++' "$OUTFILE" > "$OUTFILE.tmp" && mv "$OUTFILE.tmp" "$OUTFILE"

echo "$OUTFILE"
